{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwaryaprabhat/Advanced-RAG/blob/main/Advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download and Environment Preparation"
      ],
      "metadata": {
        "id": "KhMdMY9Ly4lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download_dataset.sh # get from https://github.com/aishwaryaprabhat/Advanced-RAG/blob/main/download_dataset.sh"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Cloning into 'DataRepository'...\nremote: Enumerating objects: 47, done.\u001b[K\nremote: Counting objects: 100% (39/39), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 47 (delta 12), reused 21 (delta 7), pack-reused 8\u001b[K\nReceiving objects: 100% (47/47), 49.80 MiB | 11.03 MiB/s, done.\nResolving deltas: 100% (12/12), done.\nUpdating files: 100% (25/25), done.\nArchive:  DataRepository/high-performance-rag/Camel Papers Test.zip\n  inflating: source_docs/Acute respiratory distress syndrome in an alpaca cria.pdf  \n  inflating: source_docs/Alpaca liveweight variations and fiber production in Mediterranean range of Chile.pdf  \nArchive:  DataRepository/high-performance-rag/Camel Papers Train.zip\n  inflating: source_docs/Antibody response to the epsilon toxin ofClostridium perfringensfollowing vaccination of Lama glamacrias.pdf  \n  inflating: source_docs/Comparative pigmentation of sheep, goats, and llamas what colors are possible through selection.pdf  \n  inflating: source_docs/Conservative management of a ruptured.pdf  \n  inflating: source_docs/Evaluation of cholesterol and vitamin E concentrations in adult alpacas and nursing crias.pdf  \n  inflating: source_docs/Influence of effects on quality traits and relationships between traits of the llama fleece..pdf  \n  inflating: source_docs/Influence of Follicular Fluid on in Vitro.pdf  \n  inflating: source_docs/Neurological Causes of Diaphragmatic Paralysis in 11 Alpacas.pdf  \n  inflating: source_docs/On the morphology of the cerebellum of the alpaca (Lama pacos)..pdf  \n  inflating: source_docs/Relationships between integumental characteristics and.pdf  \n  inflating: source_docs/Respiratory mechanics and results of cytologic examination of bronchoalveolar lavage fluid in healthy adult alpacas.pdf  \n  inflating: source_docs/Serum and urine analyte comparison between llamas and alpacas fed three forages.pdf  \n  inflating: source_docs/The physiological impact of wool-harvesting procedures in vicunas (Vicugna vicugna)..pdf  \n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOEmK9Nwyj7R",
        "outputId": "a79f7c5e-5bfb-44c2-da0d-c0989edcc0fe",
        "gather": {
          "logged": 1704857605192
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install llama-index pypdf sentence_transformers typing_extensions==4.7.1 nest_asyncio -U -q\n",
        "%pip install --upgrade -r requirements.txt -U"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Note: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "id": "qoNQu1bszvmI",
        "gather": {
          "logged": 1704858092555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# from google.colab import userdata\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-*'\n",
        "os.environ['HUGGINGFACE_API_TOKEN'] = 'hf_*'"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "T36WDvn30Pd1",
        "gather": {
          "logged": 1704863548376
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "# Initialize an embedding model from Hugging Face using the \"BAAI/bge-small-en\" model.\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "# Create an OpenAI GPT-3.5 model instance with no randomness in responses (temperature=0).\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Load data from a directory named 'source_docs' using SimpleDirectoryReader.\n",
        "source_docs = SimpleDirectoryReader('source_docs').load_data()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount() > 0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "0td9HhX66XTw",
        "gather": {
          "logged": 1704863567618
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG Techniques"
      ],
      "metadata": {
        "id": "zJ5GJYC_0t2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline 'Vanilla' RAG"
      ],
      "metadata": {
        "id": "UNHwqcmj0zQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse source_docs into nodes"
      ],
      "metadata": {
        "id": "NpL_9Bx05Cch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# Create a SimpleNodeParser instance with default settings, but with specified chunk overlap and size.\n",
        "baseline_parser = SimpleNodeParser.from_defaults(\n",
        "    chunk_overlap=200,\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "# Use the parser to extract nodes from the documents in 'source_docs'.\n",
        "baseline_nodes = baseline_parser.get_nodes_from_documents(source_docs)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "BMBcOP_C0VZJ",
        "gather": {
          "logged": 1704863573163
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# Create a ServiceContext with default settings, including the previously defined language model (llm), embedding model, and node parser.\n",
        "baseline_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, node_parser=baseline_parser)\n",
        "\n",
        "# Initialize a VectorStoreIndex with the baseline nodes and the service context.\n",
        "baseline_index = VectorStoreIndex(baseline_nodes, service_context=baseline_context)\n",
        "\n",
        "# Persist the baseline index in a directory named \"baseline_index\".\n",
        "baseline_index.storage_context.persist(persist_dir=\"baseline_index\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "SyCNfQ2a6tP-",
        "gather": {
          "logged": 1704863610711
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the baseline index into a query engine capable of finding the top 3 most similar entries.\n",
        "baseline_query_engine = baseline_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Perform a query with the baseline query engine asking about the influence of camelid genetics on wool quality.\n",
        "baseline_response = baseline_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "baseline_response.response"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'Camelid genetics can influence wool quality. The inheritance of coat colors in alpacas and llamas, which are types of camelids, has been studied. Additionally, major genes affecting alpaca fiber traits have been analyzed. The expression patterns of keratin intermediate filament and keratin associated protein genes in wool follicles have also been investigated. These studies suggest that genetic factors play a role in determining the quality of wool in camelids.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "id": "0U_1fJxH7w_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d36607b4-8d2e-4ac2-e2b6-25e783305d32",
        "gather": {
          "logged": 1704863630389
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Window Parser"
      ],
      "metadata": {
        "id": "pFrYk5bw9Yxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "\n",
        "# Initialize a SentenceWindowNodeParser with default settings, including a window size of 6 and specific metadata keys.\n",
        "sentence_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=6,\n",
        "    window_metadata_key=\"window\",\n",
        "    original_text_metadata_key=\"original_text\",\n",
        ")\n",
        "\n",
        "# Parse nodes from the documents in 'source_docs' using the sentence parser.\n",
        "sentence_nodes = sentence_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the sentence parser along with the previously defined language model and embedding model.\n",
        "sentence_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, node_parser=sentence_parser)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "ecV4IU8r9IHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204d7188-d2bc-4f72-a0dd-7323fcd21fe6",
        "gather": {
          "logged": 1704863634949
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# Create a VectorStoreIndex with the parsed sentence nodes and the defined service context.\n",
        "sentence_index = VectorStoreIndex(sentence_nodes, service_context=sentence_context)\n",
        "\n",
        "# Persist the sentence index in a directory named \"sentence_index\" for future use.\n",
        "sentence_index.storage_context.persist(persist_dir=\"sentence_index\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "1DhpZMCDuObv",
        "gather": {
          "logged": 1704863738643
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "# Convert the sentence index into a query engine, configuring it to find the top 3 most similar entries.\n",
        "# It also uses a postprocessor to replace metadata with the 'window' key values.\n",
        "sentence_query_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Perform a query using the sentence query engine about the influence of camelid genetics on wool quality.\n",
        "sentence_response = sentence_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "sentence_response.response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "'Camelid genetics influence wool quality through various mechanisms. One important aspect is coat color genetics, where llamas and alpacas exhibit a wide range of natural colors and patterns. Llamas, in particular, have greater color variation compared to alpacas. This variation is attributed to the selection process during domestication, where llamas were primarily selected for body size and fiber weight rather than color uniformity or fiber fineness. \\n\\nAdditionally, the composition and interactions of keratin intermediate filaments (KIFs) and keratin-associated proteins (KAPs) play a crucial role in determining fiber characteristics. Fiber growth in mammals, including camelids, is a cyclical process regulated by genetics, nutrition, and hormones. The proteins that form the fiber are encoded by keratin genes (KRT) and keratin-associated proteins (KRTAP), which are expressed in a highly regulated manner during hair follicle growth. \\n\\nFurthermore, genetic selection programs have been implemented to improve fleece characteristics in domestic camelids. However, the genetic mechanisms controlling fiber traits in llamas and alpacas are not fully understood. Studies have identified major genes that affect quantitative fiber traits such as fiber diameter, standard deviation of fiber diameter, variation coefficient, and comfort factor. Molecular identification of these genes would facilitate genetic improvement in wool quality. \\n\\nAnother genetic factor influencing wool quality is fiber length. The fibroblast growth factor 5 (FGF5) gene controls hair cycle and affects hair length. Alternative splicing of the FGF5 gene results in two transcripts, with the full-length form inducing the catagen phase and the short form antagonizing it during the anagen phase. Loss-of-function mutations in the FGF5 gene have been associated with long-hair phenotypes in camelids. Differences in FGF5 expression have also been observed between different regions of the body and between different types of alpacas.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gkEPhxcAusIr",
        "outputId": "d8a017b8-8aee-4d49-cd9e-283889554c97",
        "gather": {
          "logged": 1704863759971
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automerging Retrival (Using Hierarchical Nodes)"
      ],
      "metadata": {
        "id": "2J_pXwebwgbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import HierarchicalNodeParser\n",
        "\n",
        "# Initialize a HierarchicalNodeParser with default settings.\n",
        "hierarchical_parser = HierarchicalNodeParser.from_defaults()\n",
        "\n",
        "# Parse nodes from the documents in 'source_docs' using the hierarchical parser.\n",
        "hierarchical_nodes = hierarchical_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the hierarchical parser along with the previously defined language model and embedding model.\n",
        "hierarchical_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, node_parser=hierarchical_parser)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "aRMlnoTHvS3B",
        "gather": {
          "logged": 1704863768262
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, StorageContext\n",
        "\n",
        "# Create a VectorStoreIndex with the parsed hierarchical nodes and the specified service context.\n",
        "hierarchical_index = VectorStoreIndex(hierarchical_nodes, service_context=hierarchical_context)\n",
        "\n",
        "# Persist the hierarchical index in a directory named \"hierarchical_index\" for future use.\n",
        "hierarchical_index.storage_context.persist(persist_dir=\"hierarchical_index\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "-bzEmN0ZxoSq",
        "gather": {
          "logged": 1704862772165
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# Initialize an AutoMergingRetriever with the hierarchical index set as the retriever, configured for top 3 similarity matches.\n",
        "retriever = AutoMergingRetriever(hierarchical_index.as_retriever(similarity_top_k=3), storage_context=hierarchical_index.storage_context, verbose=True)\n",
        "\n",
        "# Create a RetrieverQueryEngine using the AutoMergingRetriever.\n",
        "amretriever_query_engine = RetrieverQueryEngine.from_args(retriever)\n",
        "\n",
        "# Perform a query using the AMRetriever query engine about the influence of camelid genetics on wool quality.\n",
        "amretriever_response = amretriever_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "amretriever_response.response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "'Camelid genetics play a significant role in determining wool quality. While there is still much to be understood in this field, recent advancements in genetic understanding have shed light on the genetic mechanisms that regulate economically important fiber traits in South American camelids. Mutations responsible for some monogenic or oligogenic traits have been identified, enabling molecular testing to assist breeding decisions. Additionally, the development of a 76K SNPs array for the alpaca will facilitate the identification of genes affecting more complex traits through genome-wide association studies. These advancements in genomics and the discovery of genetic variants are expected to contribute to the improvement of wool quality in camelids.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PObvkaYCx53G",
        "outputId": "4a7d11fe-3cd6-4c37-b779-3db9a4306846",
        "gather": {
          "logged": 1704864043108
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating RAG Performance"
      ],
      "metadata": {
        "id": "a09oy-4V2qbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the dataset that will be used for evaluation of RAG methods"
      ],
      "metadata": {
        "id": "RdREimVJONuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from llama_index.evaluation import (DatasetGenerator, QueryResponseDataset)\n",
        "from llama_index.evaluation import (\n",
        "    CorrectnessEvaluator,\n",
        "    SemanticSimilarityEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    FaithfulnessEvaluator\n",
        ")\n",
        "\n",
        "# Set the number of nodes to be used for evaluation.\n",
        "num_nodes_eval = 30\n",
        "\n",
        "# Randomly select a sample of nodes from baseline_nodes for evaluation.\n",
        "sample_eval_nodes = random.sample(baseline_nodes, num_nodes_eval)\n",
        "\n",
        "# Initialize a dataset generator with the sampled nodes, baseline service context, progress display enabled,\n",
        "# and generating 3 questions per chunk.\n",
        "dataset_generator = DatasetGenerator(\n",
        "    sample_eval_nodes,\n",
        "    service_context=baseline_context,\n",
        "    show_progress=True,\n",
        "    num_questions_per_chunk=3,\n",
        ")\n",
        "\n",
        "# Asynchronously generate the evaluation dataset from the nodes.\n",
        "evaluation_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
        "\n",
        "# Initialize evaluators with the baseline service context.\n",
        "correctness = CorrectnessEvaluator(service_context=baseline_context)\n",
        "semanticsimilarity = SemanticSimilarityEvaluator(service_context=baseline_context)\n",
        "relevancy = RelevancyEvaluator(service_context=baseline_context)\n",
        "faithfulness = FaithfulnessEvaluator(service_context=baseline_context)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_21824/2316716403.py:18: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n  dataset_generator = DatasetGenerator(\n\n\n\n  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n  3%|▎         | 1/30 [00:03<01:28,  3.04s/it]\u001b[A\u001b[A\u001b[A\n\n\n  7%|▋         | 2/30 [00:03<00:39,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 6/30 [00:03<00:08,  2.84it/s]\u001b[A\u001b[A\u001b[A\n\n\n 30%|███       | 9/30 [00:03<00:04,  4.71it/s]\u001b[A\u001b[A\u001b[A\n\n\n 37%|███▋      | 11/30 [00:03<00:03,  5.78it/s]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 15/30 [00:03<00:01,  9.38it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 18/30 [00:03<00:00, 12.01it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 21/30 [00:04<00:00, 12.64it/s]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 24/30 [00:04<00:00, 12.27it/s]\u001b[A\u001b[A\u001b[A\n\n\n 87%|████████▋ | 26/30 [00:04<00:00,  8.17it/s]\u001b[A\u001b[A\u001b[A\n\n\n 93%|█████████▎| 28/30 [00:05<00:00,  8.78it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 30/30 [00:06<00:00,  4.97it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.76s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:05,  2.53s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.31s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.67s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.16s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.50s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:04<00:00,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:02,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:01<00:00,  1.24it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.66s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:03,  3.16s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:10<00:00,  3.36s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:02,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:01<00:00,  1.42it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.28s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:07<00:03,  3.76s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:10<00:00,  3.62s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:05,  2.52s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:03<00:01,  1.66s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:02,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:01<00:00,  1.25it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.53s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.18s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:05,  2.89s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.70s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:06<00:00,  2.09s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.06s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.56s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.11s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:04<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:05<00:10,  5.45s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:07<00:03,  3.30s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:07<00:00,  2.50s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:05<00:10,  5.11s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.33s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:09<00:00,  3.28s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:03<00:06,  3.46s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.83s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.81s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:01<00:00,  1.17it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.68s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:04<00:02,  2.49s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:05,  2.93s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.43s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:06<00:00,  2.00s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:02,  1.08s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:01<00:00,  1.37it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.17s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.21s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:07<00:00,  2.63s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:03<00:06,  3.21s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:04<00:01,  1.92s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.12s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.61s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:06<00:00,  2.12s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.54s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:02<00:01,  1.34s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:02,  1.45s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:02<00:04,  2.42s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:03<00:01,  1.43s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.62s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 1/3 [00:01<00:03,  1.56s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 2/3 [00:05<00:02,  2.78s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 3/3 [00:09<00:00,  3.03s/it]\u001b[A\u001b[A\u001b[A\n/anaconda/envs/jupyter_env/lib/python3.8/site-packages/llama_index/evaluation/dataset_generation.py:279: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjYVX--VzQe-",
        "outputId": "0d84b8b8-1c85-4e49-b216-86f3278dccd6",
        "gather": {
          "logged": 1704865651455
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "# Define the maximum number of samples to use for evaluation.\n",
        "max_samples = 10\n",
        "\n",
        "# Extract evaluation questions from the evaluation dataset.\n",
        "evaluation_questions = evaluation_dataset.questions\n",
        "\n",
        "# Compile expected responses from the question-response pairs in the evaluation dataset.\n",
        "expected_responses = [response for (question, response) in evaluation_dataset.qr_pairs]\n",
        "\n",
        "# Create a dictionary mapping evaluation types to their respective evaluator objects.\n",
        "evaluator_dict = {\n",
        "    \"correctness\": correctness,\n",
        "    \"faithfulness\": faithfulness,\n",
        "    \"relevancy\": relevancy,\n",
        "    \"semanticsimilarity\": semanticsimilarity,\n",
        "}\n",
        "\n",
        "# Initialize a BatchEvalRunner with the evaluator dictionary, specifying 2 workers and progress display.\n",
        "batch_eval_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "id": "q2fnB4_D3hdJ",
        "gather": {
          "logged": 1704865668434
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Baseline RAG"
      ],
      "metadata": {
        "id": "ohwY2CLGP4sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation.eval_utils import get_responses, get_results_df\n",
        "\n",
        "baseline_responses = get_responses(\n",
        "    evaluation_questions[:max_samples],\n",
        "    baseline_index.as_query_engine(similarity_top_k=3),\n",
        "    show_progress=True\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "baseline_evaluation_results = await batch_eval_runner.aevaluate_responses(\n",
        "    queries=evaluation_questions[:max_samples],\n",
        "    responses=baseline_responses[:max_samples],\n",
        "    reference=expected_responses[:max_samples],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 10%|█         | 1/10 [00:02<00:20,  2.27s/it]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 2/10 [00:02<00:08,  1.07s/it]\u001b[A\u001b[A\u001b[A\n\n\n 30%|███       | 3/10 [00:02<00:04,  1.41it/s]\u001b[A\u001b[A\u001b[A\n\n\n 40%|████      | 4/10 [00:03<00:03,  1.89it/s]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 5/10 [00:03<00:03,  1.55it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 6/10 [00:04<00:01,  2.04it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 7/10 [00:04<00:01,  1.82it/s]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 8/10 [00:05<00:01,  1.78it/s]\u001b[A\u001b[A\u001b[A\n\n\n 90%|█████████ | 9/10 [00:06<00:00,  1.38it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 10/10 [00:07<00:00,  1.34it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n  2%|▎         | 1/40 [00:00<00:25,  1.54it/s]\u001b[A\u001b[A\u001b[A\n\n\n  5%|▌         | 2/40 [00:00<00:16,  2.29it/s]\u001b[A\u001b[A\u001b[A\n\n\n 12%|█▎        | 5/40 [00:01<00:08,  4.18it/s]\u001b[A\u001b[A\u001b[A\n\n\n 15%|█▌        | 6/40 [00:01<00:07,  4.64it/s]\u001b[A\u001b[A\u001b[A\n\n\n 18%|█▊        | 7/40 [00:02<00:09,  3.40it/s]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 8/40 [00:02<00:07,  4.08it/s]\u001b[A\u001b[A\u001b[A\n\n\n 22%|██▎       | 9/40 [00:02<00:10,  2.98it/s]\u001b[A\u001b[A\u001b[A\n\n\n 25%|██▌       | 10/40 [00:06<00:41,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\n 28%|██▊       | 11/40 [00:07<00:34,  1.19s/it]\u001b[A\u001b[A\u001b[A\n\n\n 32%|███▎      | 13/40 [00:08<00:22,  1.20it/s]\u001b[A\u001b[A\u001b[A\n\n\n 35%|███▌      | 14/40 [00:08<00:20,  1.25it/s]\u001b[A\u001b[A\u001b[A\n\n\n 38%|███▊      | 15/40 [00:09<00:15,  1.59it/s]\u001b[A\u001b[A\u001b[A\n\n\n 42%|████▎     | 17/40 [00:09<00:11,  2.05it/s]\u001b[A\u001b[A\u001b[A\n\n\n 45%|████▌     | 18/40 [00:12<00:24,  1.10s/it]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 20/40 [00:14<00:21,  1.05s/it]\u001b[A\u001b[A\u001b[A\n\n\n 55%|█████▌    | 22/40 [00:15<00:14,  1.23it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 24/40 [00:16<00:10,  1.52it/s]\u001b[A\u001b[A\u001b[A\n\n\n 62%|██████▎   | 25/40 [00:16<00:09,  1.54it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 28/40 [00:17<00:05,  2.05it/s]\u001b[A\u001b[A\u001b[A\n\n\n 78%|███████▊  | 31/40 [00:18<00:03,  2.72it/s]\u001b[A\u001b[A\u001b[A\n\n\n 82%|████████▎ | 33/40 [00:18<00:02,  3.47it/s]\u001b[A\u001b[A\u001b[A\n\n\n 85%|████████▌ | 34/40 [00:21<00:04,  1.27it/s]\u001b[A\u001b[A\u001b[A\n\n\n 88%|████████▊ | 35/40 [00:23<00:05,  1.05s/it]\u001b[A\u001b[A\u001b[A\n\n\n 90%|█████████ | 36/40 [00:24<00:03,  1.04it/s]\u001b[A\u001b[A\u001b[A\n\n\n 92%|█████████▎| 37/40 [00:24<00:02,  1.22it/s]\u001b[A\u001b[A\u001b[A\n\n\n 95%|█████████▌| 38/40 [00:25<00:01,  1.16it/s]\u001b[A\u001b[A\u001b[A\n\n\n 98%|█████████▊| 39/40 [00:26<00:00,  1.24it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 40/40 [00:26<00:00,  1.49it/s]\u001b[A\u001b[A\u001b[A\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsr_gJ004WRq",
        "outputId": "b7332c4f-137a-4567-ea68-030e88203efa",
        "gather": {
          "logged": 1704865706274
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = get_results_df(\n",
        "    [baseline_evaluation_results],\n",
        "    ['Baseline RAG'],\n",
        "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semanticsimilarity\"],\n",
        ")\n",
        "\n",
        "results_df.rename(columns={'names': 'RAG Method'}, inplace=True)\n",
        "\n",
        "results_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "     RAG Method  correctness  relevancy  faithfulness  semanticsimilarity\n0  Baseline RAG         4.05        0.7           0.7            0.971417",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RAG Method</th>\n      <th>correctness</th>\n      <th>relevancy</th>\n      <th>faithfulness</th>\n      <th>semanticsimilarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Baseline RAG</td>\n      <td>4.05</td>\n      <td>0.7</td>\n      <td>0.7</td>\n      <td>0.971417</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "nIPl60wGQQH7",
        "outputId": "959a1cd3-9b03-45b4-cccf-9e6a8fc69051",
        "gather": {
          "logged": 1704865712169
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Sentence Window Retrieval"
      ],
      "metadata": {
        "id": "_1JeiWMiQsIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_responses = get_responses(\n",
        "    evaluation_questions[:max_samples],\n",
        "    sentence_index.as_query_engine(similarity_top_k=3),\n",
        "    show_progress=True\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "sentence_evaluation_results = await batch_eval_runner.aevaluate_responses(\n",
        "    queries=evaluation_questions[:max_samples],\n",
        "    responses=sentence_responses[:max_samples],\n",
        "    reference=expected_responses[:max_samples],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 10%|█         | 1/10 [00:02<00:20,  2.27s/it]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 2/10 [00:02<00:10,  1.29s/it]\u001b[A\u001b[A\u001b[A\n\n\n 40%|████      | 4/10 [00:02<00:03,  1.92it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 7/10 [00:03<00:01,  2.78it/s]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 8/10 [00:04<00:00,  2.01it/s]\u001b[A\u001b[A\u001b[A\n\n\n 90%|█████████ | 9/10 [00:05<00:00,  1.84it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n  2%|▎         | 1/40 [00:00<00:21,  1.77it/s]\u001b[A\u001b[A\u001b[A\n\n\n  5%|▌         | 2/40 [00:00<00:15,  2.52it/s]\u001b[A\u001b[A\u001b[A\n\n\n 15%|█▌        | 6/40 [00:01<00:06,  5.19it/s]\u001b[A\u001b[A\u001b[A\n\n\n 18%|█▊        | 7/40 [00:03<00:18,  1.74it/s]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 8/40 [00:03<00:18,  1.75it/s]\u001b[A\u001b[A\u001b[A\n\n\n 25%|██▌       | 10/40 [00:04<00:14,  2.14it/s]\u001b[A\u001b[A\u001b[A\n\n\n 28%|██▊       | 11/40 [00:04<00:12,  2.41it/s]\u001b[A\u001b[A\u001b[A\n\n\n 32%|███▎      | 13/40 [00:05<00:08,  3.13it/s]\u001b[A\u001b[A\u001b[A\n\n\n 35%|███▌      | 14/40 [00:05<00:07,  3.57it/s]\u001b[A\u001b[A\u001b[A\n\n\n 38%|███▊      | 15/40 [00:05<00:08,  2.91it/s]\u001b[A\u001b[A\u001b[A\n\n\n 42%|████▎     | 17/40 [00:06<00:07,  3.17it/s]\u001b[A\u001b[A\u001b[A\n\n\n 45%|████▌     | 18/40 [00:06<00:08,  2.57it/s]\u001b[A\u001b[A\u001b[A\n\n\n 48%|████▊     | 19/40 [00:07<00:08,  2.47it/s]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 20/40 [00:08<00:09,  2.21it/s]\u001b[A\u001b[A\u001b[A\n\n\n 55%|█████▌    | 22/40 [00:10<00:15,  1.16it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 24/40 [00:11<00:09,  1.73it/s]\u001b[A\u001b[A\u001b[A\n\n\n 62%|██████▎   | 25/40 [00:11<00:07,  1.88it/s]\u001b[A\u001b[A\u001b[A\n\n\n 65%|██████▌   | 26/40 [00:11<00:06,  2.19it/s]\u001b[A\u001b[A\u001b[A\n\n\n 68%|██████▊   | 27/40 [00:11<00:05,  2.50it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 28/40 [00:12<00:04,  2.71it/s]\u001b[A\u001b[A\u001b[A\n\n\n 72%|███████▎  | 29/40 [00:12<00:03,  3.17it/s]\u001b[A\u001b[A\u001b[A\n\n\n 75%|███████▌  | 30/40 [00:12<00:03,  2.90it/s]\u001b[A\u001b[A\u001b[A\n\n\n 78%|███████▊  | 31/40 [00:15<00:10,  1.12s/it]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 32/40 [00:16<00:07,  1.07it/s]\u001b[A\u001b[A\u001b[A\n\n\n 85%|████████▌ | 34/40 [00:18<00:06,  1.11s/it]\u001b[A\u001b[A\u001b[A\n\n\n 88%|████████▊ | 35/40 [00:19<00:04,  1.04it/s]\u001b[A\u001b[A\u001b[A\n\n\n 90%|█████████ | 36/40 [00:19<00:03,  1.17it/s]\u001b[A\u001b[A\u001b[A\n\n\n 92%|█████████▎| 37/40 [00:20<00:02,  1.34it/s]\u001b[A\u001b[A\u001b[A\n\n\n 95%|█████████▌| 38/40 [00:21<00:01,  1.44it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 40/40 [00:21<00:00,  1.82it/s]\u001b[A\u001b[A\u001b[A\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNYPtHh5LaV",
        "outputId": "be1eeec9-9afd-44e1-e183-3d1e2e1c6933",
        "gather": {
          "logged": 1704865743901
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = get_results_df(\n",
        "    [sentence_evaluation_results],\n",
        "    ['Sentence Window Retrieval'],\n",
        "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semanticsimilarity\"],\n",
        ")\n",
        "\n",
        "results_df.rename(columns={'names': 'RAG Method'}, inplace=True)\n",
        "\n",
        "results_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "                  RAG Method  correctness  relevancy  faithfulness  \\\n0  Sentence Window Retrieval         3.85        0.9           1.0   \n\n   semanticsimilarity  \n0            0.968124  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RAG Method</th>\n      <th>correctness</th>\n      <th>relevancy</th>\n      <th>faithfulness</th>\n      <th>semanticsimilarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence Window Retrieval</td>\n      <td>3.85</td>\n      <td>0.9</td>\n      <td>1.0</td>\n      <td>0.968124</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "hQRu4HcCREvK",
        "outputId": "08f149df-d58a-428e-db55-f3b1e7c80b16",
        "gather": {
          "logged": 1704865749485
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Automerging Retrival"
      ],
      "metadata": {
        "id": "TkKB_5LjRXE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amr_responses = get_responses(\n",
        "    evaluation_questions[:max_samples],\n",
        "    amretriever_query_engine,\n",
        "    show_progress=True\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "amr_evaluation_results = await batch_eval_runner.aevaluate_responses(\n",
        "    queries=evaluation_questions[:max_samples],\n",
        "    responses=amr_responses[:max_samples],\n",
        "    reference=expected_responses[:max_samples],\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n 10%|█         | 1/10 [00:02<00:21,  2.35s/it]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 2/10 [00:02<00:09,  1.13s/it]\u001b[A\u001b[A\u001b[A\n\n\n 30%|███       | 3/10 [00:03<00:06,  1.12it/s]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 5/10 [00:03<00:02,  2.16it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 6/10 [00:03<00:01,  2.23it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 7/10 [00:04<00:01,  2.33it/s]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 8/10 [00:04<00:00,  2.29it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\u001b[A\u001b[A\u001b[A\n\n\n\n  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n  2%|▎         | 1/40 [00:00<00:20,  1.94it/s]\u001b[A\u001b[A\u001b[A\n\n\n  5%|▌         | 2/40 [00:00<00:10,  3.55it/s]\u001b[A\u001b[A\u001b[A\n\n\n  8%|▊         | 3/40 [00:01<00:15,  2.39it/s]\u001b[A\u001b[A\u001b[A\n\n\n 10%|█         | 4/40 [00:01<00:17,  2.07it/s]\u001b[A\u001b[A\u001b[A\n\n\n 12%|█▎        | 5/40 [00:04<00:46,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\n 15%|█▌        | 6/40 [00:05<00:37,  1.10s/it]\u001b[A\u001b[A\u001b[A\n\n\n 18%|█▊        | 7/40 [00:05<00:25,  1.28it/s]\u001b[A\u001b[A\u001b[A\n\n\n 22%|██▎       | 9/40 [00:06<00:17,  1.79it/s]\u001b[A\u001b[A\u001b[A\n\n\n 32%|███▎      | 13/40 [00:06<00:09,  2.97it/s]\u001b[A\u001b[A\u001b[A\n\n\n 35%|███▌      | 14/40 [00:07<00:10,  2.59it/s]\u001b[A\u001b[A\u001b[A\n\n\n 38%|███▊      | 15/40 [00:07<00:10,  2.33it/s]\u001b[A\u001b[A\u001b[A\n\n\n 42%|████▎     | 17/40 [00:08<00:08,  2.61it/s]\u001b[A\u001b[A\u001b[A\n\n\n 45%|████▌     | 18/40 [00:09<00:09,  2.31it/s]\u001b[A\u001b[A\u001b[A\n\n\n 48%|████▊     | 19/40 [00:12<00:20,  1.03it/s]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 20/40 [00:12<00:19,  1.04it/s]\u001b[A\u001b[A\u001b[A\n\n\n 52%|█████▎    | 21/40 [00:13<00:16,  1.15it/s]\u001b[A\u001b[A\u001b[A\n\n\n 55%|█████▌    | 22/40 [00:14<00:14,  1.28it/s]\u001b[A\u001b[A\u001b[A\n\n\n 60%|██████    | 24/40 [00:14<00:09,  1.68it/s]\u001b[A\u001b[A\u001b[A\n\n\n 68%|██████▊   | 27/40 [00:17<00:09,  1.32it/s]\u001b[A\u001b[A\u001b[A\n\n\n 70%|███████   | 28/40 [00:18<00:08,  1.41it/s]\u001b[A\u001b[A\u001b[A\n\n\n 72%|███████▎  | 29/40 [00:19<00:08,  1.28it/s]\u001b[A\u001b[A\u001b[A\n\n\n 80%|████████  | 32/40 [00:19<00:04,  1.91it/s]\u001b[A\u001b[A\u001b[A\n\n\n 85%|████████▌ | 34/40 [00:20<00:02,  2.23it/s]\u001b[A\u001b[A\u001b[A\n\n\n 88%|████████▊ | 35/40 [00:21<00:02,  2.12it/s]\u001b[A\u001b[A\u001b[A\n\n\n 90%|█████████ | 36/40 [00:22<00:02,  1.67it/s]\u001b[A\u001b[A\u001b[A\n\n\n 92%|█████████▎| 37/40 [00:24<00:02,  1.05it/s]\u001b[A\u001b[A\u001b[A\n\n\n 98%|█████████▊| 39/40 [00:25<00:00,  1.35it/s]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 40/40 [00:26<00:00,  1.52it/s]\u001b[A\u001b[A\u001b[A\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "> Merging 1 nodes into parent node.\n> Parent node id: 2f1ac003-dd48-497c-b343-0349d5916c7a.\n> Parent node text: The effects of different capture methods on cortisol are summarised in Table 2. The response to c...\n\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-1bCV2NRdA9",
        "outputId": "6557064a-51ce-4592-aa88-ecae59e6c273",
        "gather": {
          "logged": 1704865791290
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_results_df(\n",
        "    [amr_evaluation_results],\n",
        "    ['Automerging Retrieval'],\n",
        "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semanticsimilarity\"],\n",
        ")\n",
        "\n",
        "results_df.rename(columns={'names': 'RAG Method'}, inplace=True)\n",
        "\n",
        "results_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "                  RAG Method  correctness  relevancy  faithfulness  \\\n0  Sentence Window Retrieval         3.85        0.9           1.0   \n\n   semanticsimilarity  \n0            0.968124  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RAG Method</th>\n      <th>correctness</th>\n      <th>relevancy</th>\n      <th>faithfulness</th>\n      <th>semanticsimilarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence Window Retrieval</td>\n      <td>3.85</td>\n      <td>0.9</td>\n      <td>1.0</td>\n      <td>0.968124</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "nbBQvj2L5qkS",
        "outputId": "decf4ae4-ba59-4d9b-e7e1-841fdd53c6a4",
        "gather": {
          "logged": 1704865794261
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Results"
      ],
      "metadata": {
        "id": "ZNwxza06R_Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = get_results_df(\n",
        "    [baseline_evaluation_results, sentence_evaluation_results, amr_evaluation_results],\n",
        "    ['Baseline RAG', 'Sentence Window Retrieval', 'Automerging Retrieval'],\n",
        "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semanticsimilarity\"],\n",
        ")\n",
        "\n",
        "results_df.rename(columns={'names': 'RAG Method'}, inplace=True)\n",
        "\n",
        "results_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "                  RAG Method  correctness  relevancy  faithfulness  \\\n0               Baseline RAG         4.05        0.7           0.7   \n1  Sentence Window Retrieval         3.85        0.9           1.0   \n2      Automerging Retrieval         4.20        0.9           0.8   \n\n   semanticsimilarity  \n0            0.971417  \n1            0.968124  \n2            0.982428  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RAG Method</th>\n      <th>correctness</th>\n      <th>relevancy</th>\n      <th>faithfulness</th>\n      <th>semanticsimilarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Baseline RAG</td>\n      <td>4.05</td>\n      <td>0.7</td>\n      <td>0.7</td>\n      <td>0.971417</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence Window Retrieval</td>\n      <td>3.85</td>\n      <td>0.9</td>\n      <td>1.0</td>\n      <td>0.968124</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Automerging Retrieval</td>\n      <td>4.20</td>\n      <td>0.9</td>\n      <td>0.8</td>\n      <td>0.982428</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "mWcW_rCd50ee",
        "outputId": "d26ab89a-5a75-46ad-f5ee-92ed5b4d67cb",
        "gather": {
          "logged": 1704865796898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracking RAG Evaluation Results on MLFlow"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mlflow azureml-mlflow -U -q"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: mlflow in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (2.9.2)\nRequirement already satisfied: azureml-mlflow in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (1.54.0.post1)\nRequirement already satisfied: gunicorn<22 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (21.2.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (4.23.3)\nRequirement already satisfied: click<9,>=7.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (8.1.3)\nRequirement already satisfied: scipy<2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (1.10.1)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (6.7.0)\nRequirement already satisfied: numpy<2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (1.24.3)\nRequirement already satisfied: pandas<3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (2.0.2)\nRequirement already satisfied: cloudpickle<4 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: Jinja2<4,>=2.11 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (3.1.2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (6.0)\nRequirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (1.13.1)\nRequirement already satisfied: scikit-learn<2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (1.3.2)\nRequirement already satisfied: entrypoints<1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (0.4)\nRequirement already satisfied: querystring-parser<2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: markdown<4,>=3.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (3.1.40)\nRequirement already satisfied: Flask<4 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (3.0.0)\nRequirement already satisfied: pytz<2024 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (2023.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (0.4.4)\nRequirement already satisfied: docker<7,>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (6.1.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (2.31.0)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (0.18.0)\nRequirement already satisfied: matplotlib<4 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (3.7.4)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (2.0.25)\nRequirement already satisfied: packaging<24 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (23.0)\nRequirement already satisfied: pyarrow<15,>=4.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from mlflow) (12.0.1)\nRequirement already satisfied: msrest>=0.6.18 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (0.7.1)\nRequirement already satisfied: cryptography in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (41.0.1)\nRequirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (1.27.1)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (1.4.0)\nRequirement already satisfied: mlflow-skinny in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (2.9.2)\nRequirement already satisfied: azure-identity in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (1.15.0)\nRequirement already satisfied: azure-common<2.0.0,>=1.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (1.1.28)\nRequirement already satisfied: jsonpickle in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (3.0.1)\nRequirement already satisfied: azure-storage-blob<=12.19.0,>=12.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (12.16.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azureml-mlflow) (2.8.2)\nRequirement already satisfied: Mako in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.0)\nRequirement already satisfied: typing-extensions>=4 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (4.7.1)\nRequirement already satisfied: importlib-resources in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (5.12.0)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (1.16.0)\nRequirement already satisfied: isodate>=0.6.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azure-storage-blob<=12.19.0,>=12.5.0->azureml-mlflow) (0.6.1)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from cryptography->azureml-mlflow) (1.15.1)\nRequirement already satisfied: pyjwt>=1.7.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.7.0)\nRequirement already satisfied: tabulate>=0.7.7 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.16)\nRequirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from docker<7,>=4.0.0->mlflow) (1.6.0)\nRequirement already satisfied: blinker>=1.6.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from Flask<4->mlflow) (1.7.0)\nRequirement already satisfied: Werkzeug>=3.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from Flask<4->mlflow) (3.0.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from Flask<4->mlflow) (2.1.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.1.0)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (4.47.0)\nRequirement already satisfied: pillow>=6.2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (9.5.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (3.1.1)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from msrest>=0.6.18->azureml-mlflow) (1.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from msrest>=0.6.18->azureml-mlflow) (2023.5.7)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas<3->mlflow) (2023.3)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (3.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azure-identity->azureml-mlflow) (1.0.0)\nRequirement already satisfied: msal<2.0.0,>=1.24.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from azure-identity->azureml-mlflow) (1.26.0)\nRequirement already satisfied: pycparser in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from cffi>=1.12->cryptography->azureml-mlflow) (2.21)\nRequirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.1)\nRequirement already satisfied: portalocker<3,>=1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->azureml-mlflow) (2.7.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml import core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "import mlflow\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "\n",
        "mlflow.set_experiment(\"advanced_rag\")\n",
        "\n",
        "for index, row in results_df.iterrows():\n",
        "    with mlflow.start_run(run_name=f\"{row['RAG Method']}\"):\n",
        "        for metric in [\"correctness\", \"relevancy\", \"faithfulness\", \"semanticsimilarity\"]:\n",
        "            mlflow.log_metric(metric, row[metric])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024/01/10 05:52:48 INFO mlflow.tracking.fluent: Experiment with name 'advanced_rag' does not exist. Creating a new experiment.\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704865974555
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNA+DPd2MtegQk7gQ5AOmkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}