{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwaryaprabhat/Advanced-RAG/blob/main/Advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download and Environment Preparation"
      ],
      "metadata": {
        "id": "KhMdMY9Ly4lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download_dataset.sh # get from https://github.com/aishwaryaprabhat/Advanced-RAG/blob/main/download_dataset.sh"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Cloning into 'DataRepository'...\nremote: Enumerating objects: 47, done.\u001b[K\nremote: Counting objects: 100% (39/39), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 47 (delta 12), reused 21 (delta 7), pack-reused 8\u001b[K\nReceiving objects: 100% (47/47), 49.80 MiB | 11.03 MiB/s, done.\nResolving deltas: 100% (12/12), done.\nUpdating files: 100% (25/25), done.\nArchive:  DataRepository/high-performance-rag/Camel Papers Test.zip\n  inflating: source_docs/Acute respiratory distress syndrome in an alpaca cria.pdf  \n  inflating: source_docs/Alpaca liveweight variations and fiber production in Mediterranean range of Chile.pdf  \nArchive:  DataRepository/high-performance-rag/Camel Papers Train.zip\n  inflating: source_docs/Antibody response to the epsilon toxin ofClostridium perfringensfollowing vaccination of Lama glamacrias.pdf  \n  inflating: source_docs/Comparative pigmentation of sheep, goats, and llamas what colors are possible through selection.pdf  \n  inflating: source_docs/Conservative management of a ruptured.pdf  \n  inflating: source_docs/Evaluation of cholesterol and vitamin E concentrations in adult alpacas and nursing crias.pdf  \n  inflating: source_docs/Influence of effects on quality traits and relationships between traits of the llama fleece..pdf  \n  inflating: source_docs/Influence of Follicular Fluid on in Vitro.pdf  \n  inflating: source_docs/Neurological Causes of Diaphragmatic Paralysis in 11 Alpacas.pdf  \n  inflating: source_docs/On the morphology of the cerebellum of the alpaca (Lama pacos)..pdf  \n  inflating: source_docs/Relationships between integumental characteristics and.pdf  \n  inflating: source_docs/Respiratory mechanics and results of cytologic examination of bronchoalveolar lavage fluid in healthy adult alpacas.pdf  \n  inflating: source_docs/Serum and urine analyte comparison between llamas and alpacas fed three forages.pdf  \n  inflating: source_docs/The physiological impact of wool-harvesting procedures in vicunas (Vicugna vicugna)..pdf  \n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOEmK9Nwyj7R",
        "outputId": "a79f7c5e-5bfb-44c2-da0d-c0989edcc0fe",
        "gather": {
          "logged": 1704857605192
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install llama-index pypdf sentence_transformers typing_extensions==4.7.1 nest_asyncio -U -q\n",
        "%pip install --upgrade -r requirements.txt -U"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting llama-index==0.9.27\n  Using cached llama_index-0.9.27-py3-none-any.whl (15.8 MB)\nRequirement already satisfied: pypdf in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (3.17.4)\nRequirement already satisfied: sentence_transformers in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.2.2)\nRequirement already satisfied: typing_extensions==4.7.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (4.7.1)\nRequirement already satisfied: nest_asyncio in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.5.8)\nCollecting ragas\n  Downloading ragas-0.0.22-py3-none-any.whl (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m376.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: openai>=1.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (1.7.0)\nRequirement already satisfied: dataclasses-json in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (0.6.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: httpx in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (0.26.0)\nRequirement already satisfied: deprecated>=1.2.9.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (1.2.14)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (3.8.1)\nRequirement already satisfied: numpy in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (1.24.3)\nRequirement already satisfied: pandas in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (2.0.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (2023.12.2)\nRequirement already satisfied: requests>=2.31.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (8.2.3)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: tiktoken>=0.3.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (0.5.2)\nRequirement already satisfied: networkx>=3.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (2.0.25)\nRequirement already satisfied: typing-inspect>=0.8.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from llama-index==0.9.27->-r requirements.txt (line 1)) (0.9.0)\nRequirement already satisfied: scipy in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (1.10.1)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (0.20.2)\nRequirement already satisfied: torchvision in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (0.16.2)\nRequirement already satisfied: tqdm in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (4.66.1)\nRequirement already satisfied: sentencepiece in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (0.1.99)\nRequirement already satisfied: torch>=1.6.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (2.1.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (4.36.2)\nRequirement already satisfied: scikit-learn in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sentence_transformers->-r requirements.txt (line 3)) (1.3.2)\nCollecting pysbd>=0.3.4\n  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain\n  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting datasets\n  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (1.3.3)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (23.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.27->-r requirements.txt (line 1)) (1.9.2)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.27->-r requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from deprecated>=1.2.9.3->llama-index==0.9.27->-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: filelock in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 3)) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 3)) (6.0)\nRequirement already satisfied: packaging>=20.9 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 3)) (23.0)\nRequirement already satisfied: joblib in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.27->-r requirements.txt (line 1)) (1.3.2)\nRequirement already satisfied: click in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.27->-r requirements.txt (line 1)) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.27->-r requirements.txt (line 1)) (2023.12.25)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.10.9)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.8.0)\nRequirement already satisfied: sniffio in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (3.7.0)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from httpx->llama-index==0.9.27->-r requirements.txt (line 1)) (1.0.2)\nRequirement already satisfied: certifi in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from httpx->llama-index==0.9.27->-r requirements.txt (line 1)) (2023.5.7)\nRequirement already satisfied: idna in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from httpx->llama-index==0.9.27->-r requirements.txt (line 1)) (3.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from httpcore==1.*->httpx->llama-index==0.9.27->-r requirements.txt (line 1)) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests>=2.31.0->llama-index==0.9.27->-r requirements.txt (line 1)) (3.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests>=2.31.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.26.16)\nRequirement already satisfied: greenlet!=0.4.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.27->-r requirements.txt (line 1)) (3.0.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (2.18.1)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.105)\nRequirement already satisfied: sympy in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (1.12)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.0.106)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.105)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (10.3.2.106)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (11.0.2.54)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (8.9.2.26)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.105)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.3.1)\nRequirement already satisfied: triton==2.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (2.1.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.1.105)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (11.4.5.107)\nRequirement already satisfied: jinja2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (3.1.2)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (12.3.101)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers->-r requirements.txt (line 3)) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers->-r requirements.txt (line 3)) (0.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from dataclasses-json->llama-index==0.9.27->-r requirements.txt (line 1)) (3.20.2)\nCollecting multiprocess\n  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dill<0.3.8,>=0.3.0\n  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec[http]<=2023.10.0,>=2023.1.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from datasets->ragas->-r requirements.txt (line 6)) (12.0.1)\nCollecting pyarrow-hotfix\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nCollecting jsonpatch<2.0,>=1.33\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting langsmith<0.1.0,>=0.0.77\n  Downloading langsmith-0.0.80-py3-none-any.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain-community<0.1,>=0.0.9\n  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting langchain-core<0.2,>=0.1.7\n  Downloading langchain_core-0.1.10-py3-none-any.whl (216 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (2.8.2)\nRequirement already satisfied: tzdata>=2022.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (2023.3)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (2023.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 3)) (9.5.0)\nRequirement already satisfied: exceptiongroup in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.1.1)\nCollecting jsonpointer>=1.9\n  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\nCollecting packaging>=20.9\n  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sympy->torch>=1.6.0->sentence_transformers->-r requirements.txt (line 3)) (1.3.0)\nInstalling collected packages: xxhash, pysbd, pyarrow-hotfix, packaging, jsonpointer, fsspec, dill, multiprocess, langsmith, jsonpatch, langchain-core, llama-index, langchain-community, datasets, langchain, ragas\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.0\n    Uninstalling packaging-23.0:\n      Successfully uninstalled packaging-23.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: llama-index\n    Found existing installation: llama-index 0.9.28.post2\n    Uninstalling llama-index-0.9.28.post2:\n      Successfully uninstalled llama-index-0.9.28.post2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nazureml-core 1.51.0.post1 requires packaging<=23.0,>=20.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 dill-0.3.7 fsspec-2023.10.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.11 langchain-core-0.1.10 langsmith-0.0.80 llama-index-0.9.27 multiprocess-0.70.15 packaging-23.2 pyarrow-hotfix-0.6 pysbd-0.3.4 ragas-0.0.22 xxhash-3.4.1\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "id": "qoNQu1bszvmI",
        "gather": {
          "logged": 1705030643330
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# from google.colab import userdata\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-*'\n",
        "os.environ['HUGGINGFACE_API_TOKEN'] = 'hf_*'"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "T36WDvn30Pd1",
        "gather": {
          "logged": 1705023923185
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "# Initialize an embedding model from Hugging Face using the \"BAAI/bge-small-en\" model.\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "# Create an OpenAI GPT-3.5 model instance with no randomness in responses (temperature=0).\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Load data from a directory named 'source_docs' using SimpleDirectoryReader.\n",
        "source_docs = SimpleDirectoryReader('source_docs').load_data()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount() > 0\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4126a76cefe24be998fbfda9ce841bc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "949b8fa4f85046ce93fb0f915371b4b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9e0e75377b2482db731a02ad80b4634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2eccc994137403faef6605e6cfd4ad6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e93c1193556460491ffc15cc8b37470"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66c5273021eb411ea65a911e56dcc86d"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "0td9HhX66XTw",
        "gather": {
          "logged": 1705023962124
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG Techniques"
      ],
      "metadata": {
        "id": "zJ5GJYC_0t2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking with Overlap (Baseline)"
      ],
      "metadata": {
        "id": "UNHwqcmj0zQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse source_docs into nodes"
      ],
      "metadata": {
        "id": "NpL_9Bx05Cch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# Create an instance of SimpleNodeParser with default settings.\n",
        "# Parameters:\n",
        "#   chunk_overlap: Specifies the number of overlapping characters between adjacent text chunks.\n",
        "#                  This is useful for ensuring that context isn't lost at the boundaries of chunks.\n",
        "#   chunk_size:    Defines the size of each text chunk in characters. \n",
        "#                  This determines how the text is segmented for processing.\n",
        "baseline_parser = SimpleNodeParser.from_defaults(\n",
        "    chunk_overlap=200,  # Overlap of 200 characters between chunks\n",
        "    chunk_size=1024     # Each chunk consists of 1024 characters\n",
        ")\n",
        "\n",
        "# Use the created parser instance to extract nodes from the documents.\n",
        "# The 'get_nodes_from_documents' method processes the documents in 'source_docs'\n",
        "# and extracts structured nodes based on the parser's configuration.\n",
        "baseline_nodes = baseline_parser.get_nodes_from_documents(source_docs)\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "BMBcOP_C0VZJ",
        "gather": {
          "logged": 1705024279454
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# Create a ServiceContext instance with default settings.\n",
        "# Parameters:\n",
        "#   llm: The language model to be used within the service context. \n",
        "#        This defines how text will be interpreted or processed.\n",
        "#   embed_model: The embedding model used for converting text to numerical representations,\n",
        "#                facilitating operations like similarity search or clustering.\n",
        "#   node_parser: The node parser that structures and segments text data into manageable parts.\n",
        "#                It was defined previously in your code.\n",
        "baseline_context = ServiceContext.from_defaults(\n",
        "    llm=llm,                # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=baseline_parser   # Node parser from previous code\n",
        ")\n",
        "\n",
        "# Initialize a VectorStoreIndex with the baseline nodes and the service context.\n",
        "# The VectorStoreIndex is used for efficient storage and retrieval of vectorized text data.\n",
        "# Parameters:\n",
        "#   baseline_nodes: The nodes extracted from documents, ready for indexing.\n",
        "#   service_context: The service context that provides essential components like the embedding model.\n",
        "baseline_index = VectorStoreIndex(\n",
        "    baseline_nodes,              # Nodes to be indexed\n",
        "    service_context=baseline_context  # Service context providing necessary components\n",
        ")\n",
        "\n",
        "# Persist the baseline index in a specified directory.\n",
        "# This step saves the state of the index to disk, allowing for later retrieval or backup.\n",
        "# Parameters:\n",
        "#   persist_dir: The directory name where the index will be stored.\n",
        "baseline_index.storage_context.persist(\n",
        "    persist_dir=\"baseline_index\"  # Directory name for storing the index\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "SyCNfQ2a6tP-",
        "gather": {
          "logged": 1705024347773
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the baseline index into a query engine capable of finding the top 3 most similar entries.\n",
        "baseline_query_engine = baseline_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Perform a query with the baseline query engine asking about the influence of camelid genetics on wool quality.\n",
        "baseline_response = baseline_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "baseline_response.response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "'Camelid genetics can influence wool quality. The inheritance of coat colors in alpacas and llamas, which are types of camelids, has been studied. Additionally, major genes affecting alpaca fiber traits have been analyzed. The expression patterns of keratin intermediate filament and keratin associated protein genes in wool follicles have also been investigated. These studies suggest that genetic factors play a role in determining the quality of wool in camelids.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "id": "0U_1fJxH7w_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d36607b4-8d2e-4ac2-e2b6-25e783305d32",
        "gather": {
          "logged": 1705025880418
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Window Parser"
      ],
      "metadata": {
        "id": "pFrYk5bw9Yxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "\n",
        "# Initialize a SentenceWindowNodeParser with specific settings.\n",
        "# A SentenceWindowNodeParser is designed to parse and structure text data into nodes,\n",
        "# with a focus on sentence-level granularity.\n",
        "# Parameters:\n",
        "#   window_size: Defines the number of sentences to include in each window or node. \n",
        "#                This sets the scope of context for each node.\n",
        "#   window_metadata_key: The key under which window metadata (like window number) will be stored.\n",
        "#   original_text_metadata_key: The key for storing the original text data in the metadata.\n",
        "sentence_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=6,  # Number of sentences in each window\n",
        "    window_metadata_key=\"window\",  # Metadata key for window information\n",
        "    original_text_metadata_key=\"original_text\"  # Metadata key for original text\n",
        ")\n",
        "\n",
        "# Use the sentence parser to parse nodes from documents.\n",
        "# This method processes the documents in 'source_docs' and extracts structured nodes,\n",
        "# with each node representing a window of sentences.\n",
        "sentence_nodes = sentence_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the sentence parser and previously defined models.\n",
        "# This context will use the sentence-level node parser for processing text data,\n",
        "# along with the specified language and embedding models.\n",
        "# Parameters:\n",
        "#   llm: The language model for the service context.\n",
        "#   embed_model: The embedding model for converting text to numerical representations.\n",
        "#   node_parser: The sentence-level node parser for structuring text data.\n",
        "sentence_context = ServiceContext.from_defaults(\n",
        "    llm=llm,  # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=sentence_parser  # Sentence-level node parser\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "ecV4IU8r9IHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204d7188-d2bc-4f72-a0dd-7323fcd21fe6",
        "gather": {
          "logged": 1705024716265
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# Create a VectorStoreIndex using the parsed sentence nodes and the previously defined service context.\n",
        "# VectorStoreIndex is a structure used for efficient storage, retrieval, and manipulation of vectorized text data.\n",
        "# Parameters:\n",
        "#   sentence_nodes: The nodes obtained from parsing the documents at the sentence level.\n",
        "#                   These nodes are now ready for indexing.\n",
        "#   service_context: The service context that provides essential components (like models and parsers) for the indexing.\n",
        "sentence_index = VectorStoreIndex(\n",
        "    sentence_nodes,         # Nodes obtained from sentence-level parsing\n",
        "    service_context=sentence_context  # Service context with essential components\n",
        ")\n",
        "\n",
        "# Persist the sentence index to a directory. This step saves the current state of the index on disk.\n",
        "# It allows for the index to be reloaded and used in the future, ensuring data persistence.\n",
        "# Parameters:\n",
        "#   persist_dir: The name of the directory where the index will be stored.\n",
        "sentence_index.storage_context.persist(\n",
        "    persist_dir=\"sentence_index\"  # Directory name for storing the index\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "1DhpZMCDuObv",
        "gather": {
          "logged": 1705024847805
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "# Convert the sentence index into a query engine.\n",
        "# The query engine is configured to find the top 3 most similar entries in the index\n",
        "# when performing a query. This is useful for retrieving the most relevant information\n",
        "# based on a given input.\n",
        "# Parameters:\n",
        "#   similarity_top_k: The number of top similar entries to retrieve. Here, it's set to 3.\n",
        "#   node_postprocessors: A list of postprocessors to apply on the nodes. In this case,\n",
        "#                        a MetadataReplacementPostProcessor is used, which replaces the node's\n",
        "#                        metadata with values from the 'window' key. This can be helpful for\n",
        "#                        contextualizing results based on specific metadata.\n",
        "sentence_query_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=3,  # Find the top 3 most similar entries\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Perform a query using the sentence query engine.\n",
        "# This query is about the influence of camelid genetics on wool quality. The query engine will\n",
        "# process this input and find the most relevant entries in the index that match the query.\n",
        "sentence_response = sentence_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "# The response contains the top similar entries as determined by the query engine,\n",
        "# potentially providing insightful information about the query topic.\n",
        "response = sentence_response.response\n",
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "'Camelid genetics influence wool quality through various mechanisms. One important aspect is coat color genetics, where llamas and alpacas exhibit a wide range of natural colors and patterns. Llamas, in particular, have greater color variation compared to alpacas. This variation is attributed to the selection process during domestication, where llamas were primarily selected for body size and fiber weight rather than color uniformity or fiber fineness. \\n\\nAdditionally, the composition and interactions of keratin intermediate filaments (KIFs) and keratin-associated proteins (KAPs) play a crucial role in determining fiber characteristics. Fiber growth in mammals, including camelids, is a cyclical process regulated by genetics, nutrition, and hormones. The proteins that form the fiber are encoded by keratin genes (KRT) and keratin-associated proteins (KRTAP), which are expressed in a highly regulated manner during hair follicle growth.\\n\\nGenetic selection programs have been implemented to improve fleece characteristics in domestic camelids. However, the genetic mechanisms controlling fiber traits in llamas and alpacas are not fully understood. Studies have identified major genes that affect quantitative fiber traits such as fiber diameter, standard deviation of fiber diameter, variation coefficient, and comfort factor. Molecular identification of these genes would facilitate genetic improvement.\\n\\nFurthermore, genes such as fibroblast growth factor 5 (FGF5) play a role in regulating hair follicle growth and affecting hair length. Alternative splicing of the FGF5 gene results in different transcripts that control the catagen and anagen phases of hair growth. Loss-of-function mutations in the FGF5 gene have been associated with long-hair phenotypes in camelids. Differences in FGF5 expression have also been observed between different regions of the body and between different types of alpacas.\\n\\nOverall, camelid genetics influence wool quality through factors such as coat color variation, the composition of keratin proteins, and the regulation of hair follicle growth.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gkEPhxcAusIr",
        "outputId": "d8a017b8-8aee-4d49-cd9e-283889554c97",
        "gather": {
          "logged": 1705025864395
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automerging Retrival (Using Hierarchical Nodes)"
      ],
      "metadata": {
        "id": "2J_pXwebwgbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import HierarchicalNodeParser\n",
        "\n",
        "# Initialize a HierarchicalNodeParser with default settings.\n",
        "# HierarchicalNodeParser is designed to parse and structure text data into a hierarchy of nodes,\n",
        "# allowing for a more structured and layered representation of the text.\n",
        "# This can be particularly useful for complex documents where different levels of granularity are needed.\n",
        "hierarchical_parser = HierarchicalNodeParser.from_defaults()\n",
        "# No additional parameters are needed for default settings.\n",
        "\n",
        "# Parse nodes from the documents using the hierarchical parser.\n",
        "# This method processes the documents in 'source_docs' and extracts structured nodes,\n",
        "# organizing them hierarchically based on the document structure.\n",
        "hierarchical_nodes = hierarchical_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the hierarchical parser and previously defined models.\n",
        "# This context will now use the hierarchical node parser for processing text data,\n",
        "# in conjunction with the specified language and embedding models.\n",
        "# Parameters:\n",
        "#   llm: The language model for the service context, used for understanding and processing language data.\n",
        "#   embed_model: The embedding model for converting text into numerical representations,\n",
        "#                enabling various text analysis tasks.\n",
        "#   node_parser: The hierarchical node parser for structuring the text data.\n",
        "hierarchical_context = ServiceContext.from_defaults(\n",
        "    llm=llm,  # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=hierarchical_parser  # Hierarchical node parser\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "aRMlnoTHvS3B",
        "gather": {
          "logged": 1705025020941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, StorageContext\n",
        "\n",
        "# Create a VectorStoreIndex using the parsed hierarchical nodes and the specified service context.\n",
        "# VectorStoreIndex is used for efficient storage, retrieval, and manipulation of vectorized text data.\n",
        "# Parameters:\n",
        "#   hierarchical_nodes: The nodes obtained from parsing the documents using the hierarchical node parser.\n",
        "#                       These nodes represent the text data structured in a hierarchical format.\n",
        "#   service_context: The service context that includes essential components like models and parsers\n",
        "#                    for processing and understanding the text data.\n",
        "hierarchical_index = VectorStoreIndex(\n",
        "    hierarchical_nodes,           # Nodes structured hierarchically\n",
        "    service_context=hierarchical_context  # Service context with essential components\n",
        ")\n",
        "\n",
        "# Persist the hierarchical index to a directory. This action saves the current state of the index on disk,\n",
        "# enabling the index to be reloaded and reused in the future. It ensures the persistence and availability\n",
        "# of the indexed data for later use.\n",
        "# Parameters:\n",
        "#   persist_dir: The name of the directory where the index will be stored.\n",
        "hierarchical_index.storage_context.persist(\n",
        "    persist_dir=\"hierarchical_index\"  # Directory name for storing the index\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "-bzEmN0ZxoSq",
        "gather": {
          "logged": 1705025330992
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# Initialize an AutoMergingRetriever with the hierarchical index.\n",
        "# AutoMergingRetriever is used for retrieving data from an index by automatically merging results \n",
        "# from multiple queries or sources. It's particularly useful for complex data structures like a hierarchical index.\n",
        "# Parameters:\n",
        "#   hierarchical_index.as_retriever(similarity_top_k=3): Converts the hierarchical index into a retriever\n",
        "#                                                        configured to find the top 3 most similar entries.\n",
        "#   storage_context: Specifies the storage context from the hierarchical index for data management.\n",
        "#   verbose: Enables verbose output, providing more detailed information during retrieval operations.\n",
        "retriever = AutoMergingRetriever(\n",
        "    hierarchical_index.as_retriever(similarity_top_k=3),\n",
        "    storage_context=hierarchical_index.storage_context,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Create a RetrieverQueryEngine using the AutoMergingRetriever.\n",
        "# RetrieverQueryEngine is a query engine that uses a specified retriever for querying the indexed data.\n",
        "# It allows for complex query operations, especially in conjunction with retrievers like AutoMergingRetriever.\n",
        "amretriever_query_engine = RetrieverQueryEngine.from_args(retriever)\n",
        "\n",
        "# Perform a query using the AMRetriever query engine.\n",
        "# The query is about the influence of camelid genetics on wool quality. The query engine processes this\n",
        "# input and retrieves the most relevant entries from the index.\n",
        "amretriever_response = amretriever_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "# The response contains the results of the query as determined by the query engine, potentially offering\n",
        "# valuable insights into the queried topic.\n",
        "response = amretriever_response.response"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PObvkaYCx53G",
        "outputId": "4a7d11fe-3cd6-4c37-b779-3db9a4306846",
        "gather": {
          "logged": 1705025817121
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'Camelid genetics play a significant role in determining wool quality. While there is still much to be understood in this field, recent advancements in genetic understanding have shed light on the genetic mechanisms that regulate economically important fiber traits in South American camelids. Mutations responsible for some monogenic or oligogenic traits have been identified, allowing for molecular testing to assist breeding decisions. Additionally, the development of a 76K SNPs array for the alpaca has facilitated the identification of genes affecting more complex traits through genome-wide association studies. These advancements in genomics and the discovery of genetic variants are expected to contribute to the improvement of wool quality in camelids.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705025832194
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating RAG Performance"
      ],
      "metadata": {
        "id": "a09oy-4V2qbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating a test dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "import random\n",
        "\n",
        "# Initialize a TestsetGenerator using its default settings.\n",
        "# TestsetGenerator is used for generating test datasets, typically for model evaluation or testing.\n",
        "# The 'from_default' method sets up the generator with default configurations.\n",
        "testsetgenerator = TestsetGenerator.from_default()\n",
        "\n",
        "# Specify the sample size for the source documents.\n",
        "# This determines how many documents will be randomly selected from the source documents.\n",
        "sample_size = 10\n",
        "\n",
        "# Define the number of questions to be included in the test set.\n",
        "# This will set how many test cases or questions the test set will contain.\n",
        "num_questions = 10\n",
        "\n",
        "# Generate a test dataset from a random sample of source documents.\n",
        "# 'random.sample' is used to randomly select a subset of documents from the source.\n",
        "# The test set is then generated based on these documents.\n",
        "# Parameters:\n",
        "#   random.sample(source_docs, sample_size): A randomly selected subset of source documents.\n",
        "#   test_size: The number of questions or test cases to generate in the test set.\n",
        "testset = testsetgenerator.generate(\n",
        "    random.sample(source_docs, sample_size),  # Randomly selected documents\n",
        "    test_size=num_questions                    # Number of questions in the test set\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\n\n\n\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n 10%|█         | 1/10 [00:38<05:43, 38.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n 30%|███       | 3/10 [01:33<03:32, 30.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n 60%|██████    | 6/10 [02:36<01:38, 24.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n100%|██████████| 10/10 [03:46<00:00, 20.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n15it [04:22, 14.10s/it]                        \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n21it [05:39, 13.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n28it [07:12, 15.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705035897955
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minor cleanup and reformatting"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "test_df = testset.to_pandas()\n",
        "# Define the regex pattern to match any character that is NOT a letter, a number, '.', ',', or '?'\n",
        "pattern = r\"[^a-zA-Z0-9.,? ]\"\n",
        "\n",
        "# Define a function to replace special characters in a string\n",
        "def remove_special_chars(s):\n",
        "    return re.sub(pattern, '', str(s))\n",
        "\n",
        "# Apply the function to each cell in the DataFrame\n",
        "test_df = test_df.applymap(remove_special_chars)\n",
        "\n",
        "\n",
        "test_questions = test_df['question'].values.tolist()\n",
        "test_answers = [[item] for item in test_df['ground_truth'].values.tolist()]\n",
        "\n",
        "test_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 91,
          "data": {
            "text/plain": "                                            question  \\\n0  What is the role of the melanocortin 1receptor...   \n1  What specific integumental characteristics con...   \n2  What are the possible longterm effects of shea...   \n3  What is the role of hair in thermoregulation i...   \n4  What is the effect of ASIP mutations on melani...   \n5  What role does MC1R play in coat color regulat...   \n6  What factors contribute to the higher stress l...   \n7  In contrast, what factors contribute to lower ...   \n\n                                ground_truth_context  \\\n0   However, color inheritance in domestic South ...   \n1   SACs have developed several special integumen...   \n2   She concluded that shearing alpaca in winter ...   \n3   The specic integumental characteristics of SA...   \n4   However, if the agouti signaling protein ASIP...   \n5   For example, the mating between two white ani...   \n6   We found a strong positive correlation betwee...   \n7   We interpret this as indicating that some ind...   \n\n                                        ground_truth question_type  \\\n0  The role of the melanocortin 1receptor MC1R in...        simple   \n1  The specific integumental characteristics that...     reasoning   \n2  The possible longterm effects of shearing alpa...  multicontext   \n3  The role of hair in thermoregulation in South ...        simple   \n4  The effect of ASIP mutations on melanin produc...        simple   \n5  MC1R plays a role in coat color regulation in ...  multicontext   \n6  The factors that contribute to higher stress l...     reasoning   \n7  The factors that contribute to lower stress le...     reasoning   \n\n  episode_done  \n0         True  \n1         True  \n2         True  \n3         True  \n4         True  \n5         True  \n6        False  \n7         True  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>ground_truth_context</th>\n      <th>ground_truth</th>\n      <th>question_type</th>\n      <th>episode_done</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the role of the melanocortin 1receptor...</td>\n      <td>However, color inheritance in domestic South ...</td>\n      <td>The role of the melanocortin 1receptor MC1R in...</td>\n      <td>simple</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What specific integumental characteristics con...</td>\n      <td>SACs have developed several special integumen...</td>\n      <td>The specific integumental characteristics that...</td>\n      <td>reasoning</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the possible longterm effects of shea...</td>\n      <td>She concluded that shearing alpaca in winter ...</td>\n      <td>The possible longterm effects of shearing alpa...</td>\n      <td>multicontext</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the role of hair in thermoregulation i...</td>\n      <td>The specic integumental characteristics of SA...</td>\n      <td>The role of hair in thermoregulation in South ...</td>\n      <td>simple</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the effect of ASIP mutations on melani...</td>\n      <td>However, if the agouti signaling protein ASIP...</td>\n      <td>The effect of ASIP mutations on melanin produc...</td>\n      <td>simple</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>What role does MC1R play in coat color regulat...</td>\n      <td>For example, the mating between two white ani...</td>\n      <td>MC1R plays a role in coat color regulation in ...</td>\n      <td>multicontext</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>What factors contribute to the higher stress l...</td>\n      <td>We found a strong positive correlation betwee...</td>\n      <td>The factors that contribute to higher stress l...</td>\n      <td>reasoning</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>In contrast, what factors contribute to lower ...</td>\n      <td>We interpret this as indicating that some ind...</td>\n      <td>The factors that contribute to lower stress le...</td>\n      <td>reasoning</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 91,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705044441447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the evaluation for the 3 RAG methods across 6 metrics"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    faithfulness, \n",
        "    answer_relevancy, \n",
        "    context_precision, \n",
        "    context_recall, \n",
        "    answer_similarity, \n",
        "    answer_correctness\n",
        ")\n",
        "from ragas.llama_index import evaluate\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# List of evaluation metrics functions to be used.\n",
        "metrics = [\n",
        "    faithfulness,           # Evaluates faithfulness of the response to the source material.\n",
        "    answer_relevancy,       # Assesses relevance of the response to the query.\n",
        "    context_precision,      # Measures precision of the context in the response.\n",
        "    context_recall,         # Measures recall of the context in the response.\n",
        "    answer_correctness,     # Checks correctness of the answer.\n",
        "    answer_similarity,      # Evaluates similarity of the answer to a reference answer.\n",
        "]\n",
        "\n",
        "# A list to collect individual result DataFrames.\n",
        "results_list = []\n",
        "\n",
        "# A list of tuples, each containing a query engine and its corresponding technique name.\n",
        "indices = [\n",
        "    (baseline_query_engine, 'chunks_with_overlap'),\n",
        "    (sentence_query_engine, 'sentence_window'),\n",
        "    (amretriever_query_engine, 'hierarchical_automerge')\n",
        "]\n",
        "\n",
        "# Iterate over each query engine and technique pair.\n",
        "for query_engine, technique in indices:\n",
        "    # Evaluate the query engine.\n",
        "    result = evaluate(query_engine, metrics, test_questions, test_answers)\n",
        "\n",
        "    # Add a 'technique' column to the result DataFrame.\n",
        "    result['technique'] = technique\n",
        "\n",
        "    # Add the result DataFrame to the results list.\n",
        "    results_list.append(result)\n",
        "\n",
        "    # Sleep to handle rate limits.\n",
        "    time.sleep(60)"
      ],
      "outputs": [],
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705039418250
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each Result object's items to a dictionary and collect them in a list\n",
        "dict_list = [dict(result.items()) for result in results_list]\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "results_df = pd.DataFrame(dict_list)\n",
        "\n",
        "results_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 88,
          "data": {
            "text/plain": "   faithfulness  answer_relevancy  context_precision  context_recall  \\\n0      0.718750          0.864502           0.625000          0.9250   \n1      0.852083          0.990066           0.802083          0.8875   \n2      0.843750          0.962274           0.937500          0.8750   \n\n   answer_correctness  answer_similarity               technique  \n0            0.557648           0.964971     chunks_with_overlap  \n1            0.620310           0.977860         sentence_window  \n2            0.557285           0.962493  hierarchical_automerge  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>faithfulness</th>\n      <th>answer_relevancy</th>\n      <th>context_precision</th>\n      <th>context_recall</th>\n      <th>answer_correctness</th>\n      <th>answer_similarity</th>\n      <th>technique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.718750</td>\n      <td>0.864502</td>\n      <td>0.625000</td>\n      <td>0.9250</td>\n      <td>0.557648</td>\n      <td>0.964971</td>\n      <td>chunks_with_overlap</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.852083</td>\n      <td>0.990066</td>\n      <td>0.802083</td>\n      <td>0.8875</td>\n      <td>0.620310</td>\n      <td>0.977860</td>\n      <td>sentence_window</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.843750</td>\n      <td>0.962274</td>\n      <td>0.937500</td>\n      <td>0.8750</td>\n      <td>0.557285</td>\n      <td>0.962493</td>\n      <td>hierarchical_automerge</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705039375583
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracking RAG Evaluation Results on MLFlow"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mlflow azureml-mlflow -U -q"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "import mlflow\n",
        "\n",
        "# Load the Azure ML workspace configuration\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Set the MLflow tracking URI\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "\n",
        "# Set the MLflow experiment name\n",
        "mlflow.set_experiment(\"advanced_rag_eval\")\n",
        "\n",
        "# Assuming 'technique' is the column name in results_df that stores the technique name\n",
        "# And other columns in results_df are the metrics you want to log\n",
        "for index, row in results_df.iterrows():\n",
        "    # Start a new MLflow run\n",
        "    with mlflow.start_run(run_name=f\"{row['technique']}\"):  # Use 'technique' column to name the run\n",
        "        # Log each metric in the row\n",
        "        for metric in row.index:\n",
        "            if metric != 'technique':  # Exclude the 'technique' column from metrics\n",
        "                mlflow.log_metric(metric, row[metric])\n",
        "\n",
        "# Note: Adjust the column names if they are different in your DataFrame\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Bad pipe message: %s [b'!\\xf4o\"`\\x83F\\xd8?d]\\xbe\\xff\\xdfmO(\\x8c \\xce\\x1a\\xda\\xa8kdG\\xea\\x05\\x94#)Z\\xf9K\\xd8\\xefJ\\xffx\\xe1\\xe8\\xaf\\xf1.\\xd9\\xa9:Q\\xee%\\xee\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06', b'\\x05\\x01\\x06', b'']\nBad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xf41P\\xf6\\xcet+9\\xaa*\\xce\\xd5\\x947\\xae\\x82F\\xb2\\x12\\xa5\\xc0/']\nBad pipe message: %s [b'\\xc6VBm\\xe7\\xd4\\x15:w_\\x85)\\x0cy\\x01\\x03SW ', b\"E\\xa4\\xc6*:\\xe9\\xad\\xc4\\xea'\\xa3iRV\\xe2d\\xfc:f\\xec(:\\xba]\\xfc\\xfc\\x1a\\xf2$V\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0\"]\nBad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04']\nBad pipe message: %s [b'\\x03\\x06', b'\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\nBad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 n\\xad\\xdch\\x84\\x16,/z\\xfe\\xae\\x13t\\xb7\\x94\\xbd|\\xd7\\x19\\xa2\\x0ce']\nBad pipe message: %s [b\"c\\xbd-V\\xf6\\xa7\\xe6\\xfb\\x13\\xc0\\t\\x82\\xb4\\x9b\\xfe'e\\x1a\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0\"]\nBad pipe message: %s [b'\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05', b'\\x03\\x08']\nBad pipe message: %s [b'\\x08\\x08\\t\\x08\\n\\x08']\nBad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b\"\\xfbyoI\\xc1\\x189\\xff\\xd0\\x1e35D6=\\xb6\\x15\\x94\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\"]\nBad pipe message: %s [b'\\xfe\\xba\\xe4(\\xa0\\xbd\\xe3\\x9dv\\xc0>\\xf1B\\x886\\x8aH\\xd0\\x00\\x00>\\xc0\\x14\\xc0']\nBad pipe message: %s [b'9\\x008\\x007\\x006\\xc0\\x0f']\nBad pipe message: %s [b'|\\xda6\\xe5\\x0f\\xbe\\t\\x0c\\xef\\x9b)\\xe2\\xc8 \\xe21=\\xb9\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0', b'3\\x002\\x001\\x000\\x00']\nBad pipe message: %s [b'\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00']\nBad pipe message: %s [b'\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\nBad pipe message: %s [b'\\xd7mH\\x8e\\xc0\\xe8!\\xbb\\xb4\\x8d\\xca\\x82\\t \\xe3p\\xa1O\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.']\nBad pipe message: %s [b'0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00']\nBad pipe message: %s [b'\\xc0\\xda\\x15x*G\\xfcQ.T\\x83r3D\\x07X\\xa2\\x0c\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r']\nBad pipe message: %s [b'\\x0fVl\\xf3k\\xf7\\xf24t!\\xca{\\xd7?D\\x93Q\\x0b\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00', b\"j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\"]\nBad pipe message: %s [b'\\x04\\x00\\xff\\x02']\nBad pipe message: %s [b'']\nBad pipe message: %s [b\"!\\x8e\\xdc[31\\xf3\\x7f\\xf8\\x0f-\\xb9\\x96Gx\\xf5\\xdf\\xc3\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\"]\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2024/01/12 06:13:43 INFO mlflow.tracking.fluent: Experiment with name 'advanced_rag_eval' does not exist. Creating a new experiment.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        }
      ],
      "execution_count": 90,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1705040027757
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNA+DPd2MtegQk7gQ5AOmkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}